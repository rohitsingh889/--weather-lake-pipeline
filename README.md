# ğŸŒ¦ Incremental Serverless Weather Data Lake Pipeline on AWS

A production-style serverless data engineering pipeline that ingests historical weather data from the Open-Meteo API, stores raw data in Amazon S3, performs Spark-based transformations using AWS Glue (PySpark), generates analytics-ready datasets, and enables SQL querying via AWS Glue Data Catalog and Amazon Athena â€” fully orchestrated by Apache Airflow.

---

## ğŸš€ Project Overview

This project demonstrates a modern **serverless data lake architecture** on AWS using industry-standard data engineering patterns:

âœ” API-Driven Data Ingestion  
âœ” Bronze / Silver / Gold Data Lake Design  
âœ” Incremental Processing Strategy  
âœ” Spark Transformations with AWS Glue  
âœ” Data Quality Validation  
âœ” Metadata Management with Glue Crawler  
âœ” Serverless SQL Analytics via Athena  
âœ” Workflow Orchestration using Airflow  

---

## ğŸ— Architecture

Pipeline Flow:

Open-Meteo Weather API  
â†’ Python Extraction (Requests + Boto3)  
â†’ Amazon S3 (Bronze Layer â€“ Raw JSON)  
â†’ AWS Glue Job (Silver â€“ PySpark Transformations + Data Quality Checks)  
â†’ Amazon S3 (Silver Layer â€“ Parquet)  
â†’ AWS Glue Job (Gold â€“ Aggregations)  
â†’ Amazon S3 (Gold Layer â€“ Analytics Ready Parquet)  
â†’ AWS Glue Crawler  
â†’ AWS Glue Data Catalog  
â†’ Amazon Athena  
â†’ BI / Analytics Dashboard  

---

## ğŸ§© Technologies Used

- **AWS S3** â†’ Data Lake Storage  
- **AWS Glue** â†’ Serverless Spark ETL (PySpark)  
- **AWS Glue Crawler** â†’ Schema & Metadata Discovery  
- **AWS Glue Data Catalog** â†’ Table Definitions for Athena  
- **Amazon Athena** â†’ Serverless SQL Query Engine  
- **Apache Airflow** â†’ Pipeline Orchestration  
- **Python** â†’ API Ingestion & S3 Upload  
- **Boto3** â†’ AWS SDK for Python  
- **Requests Module** â†’ REST API Calls  

---

## ğŸ“¡ Data Source

**API Provider:** Open-Meteo Archive API  

The pipeline retrieves **historical hourly weather data** including:

- Temperature  
- Precipitation  
- Windspeed  
- Timestamp  

Data is fetched dynamically for configured cities.

---

## âš™ Extraction Layer (Python)

Data ingestion is handled via Python scripts using:

### âœ… `requests` module
Used to make REST API calls to Open-Meteo.

### âœ… `boto3` (AWS SDK)
Used to upload raw JSON responses directly into Amazon S3.

Example responsibilities:

âœ” Fetch previous day's weather data  
âœ” Preserve raw API response  
âœ” Store immutable JSON in Bronze layer  

---

## ğŸ—‚ Bronze Layer â€“ Raw Zone

**Storage:** Amazon S3  
**Format:** Raw JSON  
**Partitioning Strategy:**

```
bronze/weather/
    city=XYZ/
        year=YYYY/
            month=MM/
                day=DD/
```

Purpose:

âœ” Preserve original API data  
âœ” Allow replay & debugging  
âœ” Maintain auditability  

No transformations occur here.

---

## ğŸ”„ Silver Layer â€“ Transformation Zone

**Processing Engine:** AWS Glue (PySpark)

Responsibilities:

âœ” Flatten nested JSON arrays  
âœ” Parse timestamps  
âœ” Cast numeric fields  
âœ” Remove duplicates  
âœ” Apply Data Quality Checks  

### âœ… Data Quality Validations

- Null Checks  
- Domain Range Checks  
- Duplicate Detection  
- Fail-Fast Mechanism  

Output Format:

âœ” **Parquet (Columnar, Optimized)**

Partition Strategy:

```
silver/weather/date=YYYY-MM-DD/
```

Benefits:

âœ” Faster Athena queries  
âœ” Reduced scan cost  
âœ” Analytics-friendly layout  

---

## ğŸ“Š Gold Layer â€“ Analytics Zone

**Processing Engine:** AWS Glue (Aggregation Job)

Responsibilities:

Transform hourly records â†’ Daily city-level metrics

Generated Metrics:

- Average Temperature  
- Maximum Temperature  
- Total Precipitation  
- Average Windspeed  

Output:

âœ” Parquet  
âœ” Partitioned by date  

```
gold/weather/date=YYYY-MM-DD/
```

Purpose:

âœ” BI / Dashboard consumption  
âœ” Small & efficient datasets  
âœ” Business-ready structure  

---

## ğŸ” Incremental Processing Strategy

The pipeline follows a **partition-level incremental model**.

Behavior:

âœ” Processes only the target `process_date`  
âœ” Overwrites only that partition  
âœ” Safe re-runs (idempotent)  
âœ” Prevents duplicates  

Mechanism:

```python
.mode("overwrite")
.option("replaceWhere", "date = 'YYYY-MM-DD'")
```

Industry-standard pattern âœ”

---

## â›“ Orchestration Layer â€“ Apache Airflow

Apache Airflow controls the workflow execution order:

âœ” API Extraction  
âœ” Silver Glue Job  
âœ” Gold Glue Job  
âœ” Glue Crawler  

Airflow runs inside a **Dockerized local environment**, simulating real-world orchestration setups.

Benefits:

âœ” Clear dependency management  
âœ” Retry & failure handling  
âœ” Cloud job coordination  

---

## ğŸ³ Dockerized Airflow Environment

Airflow is deployed locally using Docker for:

âœ” Environment isolation  
âœ” Reproducibility  
âœ” Easy dependency management  

This mimics production orchestration patterns without managing servers.

---

## ğŸ§¾ Metadata & Query Layer

### âœ… AWS Glue Crawler
Automatically infers schema from Parquet datasets.

### âœ… AWS Glue Data Catalog
Stores table definitions used by Athena.

### âœ… Amazon Athena
Executes SQL queries directly on S3 data.

Advantages:

âœ” Fully serverless  
âœ” No cluster management  
âœ” Cost-efficient analytics  

---

## ğŸ“ˆ Analytics / BI Layer

Athena-queryable Gold datasets can be consumed by:

âœ” BI dashboards  
âœ” SQL clients  
âœ” Visualization tools  

---

## ğŸ“ Project Structure

Airflow DAG environment contains:

âœ” DAG file  
âœ” API client logic  
âœ” Extraction logic  
âœ” S3 writer logic  

All Python ingestion modules reside in the **same Airflow DAG location**, ensuring easy imports and simplified orchestration.

AWS Glue jobs execute independently within AWS.

---

## âœ… Key Engineering Concepts Demonstrated

- Serverless Data Lake Architecture  
- Incremental Data Processing  
- Partition-Aware Storage Design  
- Spark Transformations (PySpark)  
- Data Quality Enforcement  
- Metadata-Driven Analytics  
- Workflow Orchestration  

---

## ğŸ‘¨â€ğŸ’» Author

**Rohit Raj Singh**

---

## â­ Why This Project Matters

This pipeline mirrors **real data engineering workflows** used in production systems:

âœ” API ingestion pipelines  
âœ” Cloud-native ETL design  
âœ” Analytics-optimized storage  
âœ” Failure-resilient processing  

Designed for learning **industry-relevant AWS data engineering practices**.

---
